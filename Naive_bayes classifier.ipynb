{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://images.unsplash.com/photo-1466096115517-bceecbfb6fde?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=427bcc1d8e2505d31a239d0de6b13f75&auto=format&fit=crop&w=1950&q=80\"  width=\"900\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Problem statement:** classify SMS messages as *HAM* or *SPAM* using **naive bayes** in supervised machine setting.\n",
    "See this link to get an idea supervised learning workflow [supervsed learning workflow](http://www.allprogrammingtutorials.com/tutorials/introduction-to-machine-learning.php)\n",
    "\n",
    "**Dataset:** We will use [SMS Spam Collection Data Set](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) from UCI machine learning repository.\n",
    "\n",
    "credit:\n",
    "\n",
    "- some of the images are from https://cdn.pixabay.com\n",
    "- https://unsplash.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this notebook may require installation of gensim  NLP(text processing) library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output should be 0 after successful install\n",
    "# run this only once. Comment later\n",
    "os.system('pip install gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18482\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "#Must for inline plot\n",
    "%matplotlib inline \n",
    "import requests\n",
    "import numpy as np\n",
    "import pprint # for pretty printing\n",
    "import zipfile # for zip and unzip utilities\n",
    "import pandas # for data analysis\n",
    "import csv\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "import seaborn as sb\n",
    "import gensim\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "r = requests.get(data_url)\n",
    "#r.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download and save the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_zip_file = 'smsspamcollection.zip'\n",
    "with open(sms_zip_file, 'wb') as out_file:\n",
    "    out_file.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's verify it. \n",
    "**make sure output of following command contains smsspamcollection.zip file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'data', 'Hands on Machine Learning with Scikit Learn and TensorFlow.pdf', 'HW1.ipynb', 'HW1.py', 'HW2f.ipynb', 'HW6.ipynb', 'Kernel PCA.ipynb', 'LDA_MNIST.ipynb', 'MNIST_data', 'Naive_bayes classifier.ipynb', 'PCA_MNIST.ipynb', 'ridge_regression_code.ipynb', 'smsspamcollection.zip', 'Sols.pdf', 'Text.pdf']\n"
     ]
    }
   ],
   "source": [
    "#Let verify it. \n",
    "dir_listing = os.listdir('.') # list content of current directory\n",
    "print(dir_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(sms_zip_file,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's list the content of the new data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['readme', 'SMSSpamCollection']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir('./data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMSSpamCollection file contains around 5k SMS messages. Checkout readme file for details.\n",
    "\n",
    "**Let's open this file and store line in python list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with  open('./data/SMSSpamCollection', 'r') as f:\n",
    "    sms_messages = f.readlines()\n",
    "#print(sms_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Following code show how to write list comprehension. We could have done this using for loop too.\n",
    "# [<some_func>(x) for x in <something> if  <some_condition_is_true>]\n",
    "sms_messages = [m.rstrip() for m in sms_messages] # we are not using if condition part\n",
    "#print('Number of sms messages is {}'.format(len(sms_messages)))\n",
    "#print(sms_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's check couple of messages again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message id 0  ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "message id 1  ham\tOk lar... Joking wif u oni...\n",
      "message id 2  spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "message id 3  ham\tU dun say so early hor... U c already then say...\n",
      "message id 4  ham\tNah I don't think he goes to usf, he lives around here though\n",
      "message id 5  spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv\n",
      "message id 6  ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "message id 7  ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "message id 8  spam\tWINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "message id 9  spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "message id 10  ham\tI'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\n",
      "message id 11  spam\tSIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info\n",
      "message id 12  spam\tURGENT! You have won a 1 week FREE membership in our Â£100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18\n",
      "message id 13  ham\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\n",
      "message id 14  ham\tI HAVE A DATE ON SUNDAY WITH WILL!!\n",
      "message id 15  spam\tXXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL\n",
      "message id 16  ham\tOh k...i'm watching here:)\n",
      "message id 17  ham\tEh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.\n",
      "message id 18  ham\tFine if thatÂ’s the way u feel. ThatÂ’s the way its gota b\n",
      "message id 19  spam\tEngland v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/Ãº1.20 POBOXox36504W45WQ 16+\n"
     ]
    }
   ],
   "source": [
    "for idx, msg in enumerate(sms_messages[0:20]): # see how we can slice list using : operator\n",
    "    print('message id {}  {}'.format(idx, msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is our  data set $\\mathcal{D} = \\{({x_i}, y_i)\\}_{i=1}^{N=5574}$ $x_i$ is sms message and $y_i$ is label(ham or spam)**. Using  this we will train(learn parameters $\\theta$ of a models(Naive bayes etc.)) and use trained model to classify new messages as ham or spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrapping the file in pandas simplify lot of tasks\n",
    "messages = pandas.read_csv('./data/SMSSpamCollection', sep='\\t', quoting=csv.QUOTE_NONE,\n",
    "                           names=[\"label\", \"message\"])\n",
    "messages.head(6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to understand various attribute of the data\n",
    "\n",
    "*How many messages in each group etc.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4827</td>\n",
       "      <td>4518</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4827   4518                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As you can see  spam class has less number of example than ham class. This is called **class imbalance** issue. \n",
    "- We need to be carfeful in machine learning application about class imbalance.\n",
    "\n",
    "This can be handled by :\n",
    "\n",
    "Up-sampling minority class:\n",
    "This is done by randomly duplicating observations from the minority class in order to reinforce its signal.\n",
    "\n",
    "Down-sampling majority class:\n",
    "This is done by randomly removing observations from the majority class to prevent its signal from dominating the learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "We need to convert text to vectors(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we'll use the [Bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model) approach for creating feature\n",
    "representing our sms messages.\n",
    "\n",
    "### Bag of word model for document:\n",
    "\n",
    "In BOG  we treat document as collection of word without any order, like they are lying in bag. \n",
    "\n",
    "Two model to represent sms/document in a vector form are:\n",
    "\n",
    "- **Bernoulli document model: mes**sage is represented by a binary feature vector of absence or presence of word.\n",
    "- **Multinomial document model**: message is represented by an integer feature vector of word frequency.\n",
    "\n",
    "\n",
    "\n",
    "We will use **Multinomial document model** in this exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert a message into vector we need to:\n",
    "\n",
    "1. convert a sentence into word token\n",
    "2. Normalize the words i.e do we care about Capital form(Cow vs cow), inflected form (\"goes\" vs. \"go\")\n",
    "3. Build a dictionary of words and map the messages into vector using this dictionary\n",
    "4. Finally train a  Naive Bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will use a python library [Gensim](https://radimrehurek.com/gensim/tutorial.html) to do heavy lifting for us.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_messages = []\n",
    "for c in messages.message:\n",
    "    preprocessed_messages.append(gensim.utils.simple_preprocess(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_messages=  np.array(preprocessed_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5574"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Ok lar... Joking wif u oni...\n",
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n"
     ]
    }
   ],
   "source": [
    "for m in messages.message[0:3]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'great', 'world', 'la', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']),\n",
       "       list(['ok', 'lar', 'joking', 'wif', 'oni']),\n",
       "       list(['free', 'entry', 'in', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'to', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'apply', 'over'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_messages[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's partition our data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_labels = messages.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples are 5574\n",
      "train set size is 5016 test set size is 558\n"
     ]
    }
   ],
   "source": [
    "training_set_portion =.9 # keep 90 % data for training\n",
    "#LEt's create some random integer index and partition the data\n",
    "number_of_examples = len(preprocessed_messages)\n",
    "print('Total examples are {}'.format(number_of_examples))\n",
    "np.random.seed(0) # to make sure multiple run give same result\n",
    "random_index = np.random.permutation(range(number_of_examples))\n",
    "training_set_size = int(number_of_examples*training_set_portion)\n",
    "print('train set size is {} test set size is {}'.format(training_set_size,number_of_examples - training_set_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['ok', 'lar', 'joking', 'wif', 'oni']),\n",
       "       list(['free', 'entry', 'in', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', 'text', 'fa', 'to', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 'apply', 'over']),\n",
       "       list(['dun', 'say', 'so', 'early', 'hor', 'already', 'then', 'say'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_messages[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training X (5016,) and train Y (5016,)\n",
      "Shape of test X (558,) and test Y (558,)\n"
     ]
    }
   ],
   "source": [
    "training_messages = preprocessed_messages[random_index[:training_set_size]]\n",
    "training_labels = messages_labels[random_index[:training_set_size]]\n",
    "test_messages = preprocessed_messages[random_index[training_set_size:]]\n",
    "test_labels = messages_labels[random_index[training_set_size:]]\n",
    "\n",
    "print('Shape of training X {} and train Y {}'.format(training_messages.shape, training_labels.shape))\n",
    "print('Shape of test X {} and test Y {}'.format(test_messages.shape, test_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll use training messages only for building the model**\n",
    "\n",
    "- We need to convert each message into count vector. Where a ham/spam message is mapped to vector representing each word frequency in the message\n",
    "    + To do this we need to build a dictionary of words first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many unique words are in our dictionary\n",
    "unique_word = set()\n",
    "for message in training_messages:\n",
    "    unique_word.update(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7353"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many words in vocabulary(V)\n",
    "len(unique_word) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will encode each message into len(unique_word) dimensional vector. In meachine learning we call acitivity like this feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let' use default dictionary to assign each word a unique location in feature vector\n",
    "from collections import defaultdict, Counter\n",
    "word_to_index_dict = defaultdict(int)\n",
    "for index , word in enumerate(unique_word):\n",
    "    word_to_index_dict[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a reverse dictionary  for mapping index to word. It will help in debugging etc.\n",
    "# See how we used dictionary comprehension\n",
    "index_to_word_dict = { value:key  for key, value in word_to_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5016,)\n"
     ]
    }
   ],
   "source": [
    "print(training_messages.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we will convert each  messages into |V| dimensional vector, where |V| is size of our dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create a numpy integer matrix of right size, initialized with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5016, 7353)\n"
     ]
    }
   ],
   "source": [
    "# each row in training_X is our x_i\n",
    "training_X = np.zeros((len(training_messages), len(unique_word)), dtype=int)\n",
    "print(training_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using [Counter](https://docs.python.org/3.5/library/collections.html#collections.Counter) from collections to count frequency of each word in a message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go over each training message, count the words using Counter and set count in feature vector for sms \n",
    "for sms_no, sms in enumerate(training_messages):\n",
    "    word_freq = Counter(sms)\n",
    "    # setting the word count in sms_no row of sms_features\n",
    "    for word, freq in word_freq.items():\n",
    "            index_of_word = word_to_index_dict[word]\n",
    "            training_X[sms_no][index_of_word] = freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing code is easy :)\n",
    "## But how we check if it is correct\n",
    "## Let's do some primitive checking on a sms message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'if': 2, 'you': 2, 'ok': 1, 'can': 1, 'be': 1, 'later': 1, 'showing': 1, 'around': 1, 'want': 1, 'cld': 1, 'have': 1, 'drink': 1, 'before': 1, 'wld': 1, 'prefer': 1, 'not': 1, 'to': 1, 'spend': 1, 'money': 1, 'on': 1, 'nosh': 1, 'don': 1, 'mind': 1, 'as': 1, 'doing': 1, 'that': 1, 'nxt': 1, 'wk': 1})\n",
      "##Encoding for sms no 3 in feature vector is ##\n",
      "to 1\n",
      "showing 1\n",
      "nosh 1\n",
      "can 1\n",
      "prefer 1\n",
      "on 1\n",
      "as 1\n",
      "mind 1\n",
      "before 1\n",
      "you 2\n",
      "if 2\n",
      "later 1\n",
      "around 1\n",
      "doing 1\n",
      "money 1\n",
      "don 1\n",
      "have 1\n",
      "be 1\n",
      "drink 1\n",
      "ok 1\n",
      "that 1\n",
      "not 1\n",
      "cld 1\n",
      "wk 1\n",
      "nxt 1\n",
      "wld 1\n",
      "want 1\n",
      "spend 1\n"
     ]
    }
   ],
   "source": [
    "sms_no =3\n",
    "message_word_count = Counter(training_messages[sms_no])\n",
    "print(message_word_count)\n",
    "\n",
    "# Let' check non zero location in sms_features to see if count is set properly\n",
    "print('##Encoding for sms no {} in feature vector is ##'.format(sms_no))\n",
    "for i, count in enumerate(training_X[sms_no]):\n",
    "    if count >0:\n",
    "        print(index_to_word_dict[i], count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have successfully converted sms message into feature vector and\n",
    "# collected them in numpy matrix\n",
    "\n",
    "<img src=\"https://images.unsplash.com/photo-1522098543979-ffc7f79a56c4?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=3deb7fa95bb0a7343a38b724cbee4b5a&auto=format&fit=crop&w=1868&q=80\" alt=\"Well done\" width=\"500\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's convert ham and spam label to 1 and 0  respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1201     ham\n",
       "4260     ham\n",
       "424     spam\n",
       "4421     ham\n",
       "3715     ham\n",
       "664      ham\n",
       "350      ham\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels.tail(7)# can check from head too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5016,)\n"
     ]
    }
   ],
   "source": [
    "# This is our training_y value\n",
    "training_y = (training_labels.values == 'ham').astype(int)\n",
    "print(training_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check some lable value\n",
    "training_y[-7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model or estimating parameters $\\theta$ of the model\n",
    "Now we have vector feature representation $x_i$ of our sms samples. \n",
    "\n",
    "Let review some theory and see what parameters we need to estimate for Naive bayes model.\n",
    "\n",
    "We know that we classify a sms $x_i$  to a class c= HAM or c= SPAM which has maximum value of $P(c|x_i).$ Using bayes rule we have $P(c|x_i) = \\frac{P(x_i|c) P(c)}{P(x_i)} \\propto P(x_i|c) P(c)$ as normalization doesn't depend on class label. \n",
    "\n",
    "In naive bayes assumption for modelling class conditional densities we have $P(x_i|c) = \\prod_j^D P(x_{ij}|c)$ assuming  $x_i \\in \\mathbb{R}^D$ i.e each example has $D$ dimentional features.\n",
    "\n",
    "**Note:$D$ is size of our vacabulary ($|V|$) build from sms document corpus i.e D = |V|**\n",
    "\n",
    "**what probability distribution we should choose for $P(x_{ij}|c)?$ **\n",
    "\n",
    "Each value $x_{ij}$ is an integer values(count of words) and there are total $D$ different unique values(word). This definetly suits a **$D$ side die** situation. In our case die has $D = |V|$ sides= size of feature vector.\n",
    "\n",
    "**Infact once we have learned $P(x_{ij}|c)?$ i.e probabilites of different sides for ham and spam die,**\n",
    "** ham or spam sms generation in bag of word model is nothing but rolling ham or spam die. Pick the word dictated by the side of die throw.**\n",
    "\n",
    "Now we  know that we can put multinomial distribution for such situation. Hence\n",
    "<font size = 6> \n",
    "$P(x_i|c) = \\frac{n !}{\\prod_j^D x_{ij !}} P(c) \\prod^{D} P(w_j|c)^{x_{ij}} \\propto P(c) \\prod^{D} P(w_j|c)^{x_{ij}}$ \n",
    "</font>\n",
    "as normalization doesn't depend on class label\n",
    "\n",
    "We know that using MLE estimate we have\n",
    "<font size = 8> \n",
    "$P(w_j|c) = \\frac{\\sum_{i=1}^N x_{ij}\\mathbb{1}(y_i=c)}{\\sum_{k=1}^{D} \\sum_{i=1}^N x_{ik}\\mathbb{1}(y_i=c)}.$ \n",
    "</font>\n",
    "where $\\mathbb{1}$ is indicator function.\n",
    "\n",
    "\n",
    "- Hence the parameters are nothing  nothing but relative frequency of $w_j$ in documents of class c=SPAM or c= HAM\n",
    "with respect to the total number of words in documents of that class.\n",
    "\n",
    "- We can sum our numpy sms_feature matrix along row or dim 0 to get total frequency of each feature for ham and spam class\n",
    "- normalize total frequency of each feature with total frequency of all the features for each class.\n",
    "- prior class  densites are estimated as $P(c) = \\frac{N_c}{N}.$ Where $N_c$ are numer of document in class k.\n",
    "\n",
    "\n",
    "\n",
    "# Let's learn the parameters for c= ham(1) and c= spam(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For ham class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4346, 7353)\n",
      "(7353,)\n",
      "56547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.76844041e-05, 1.60928078e-03, 1.76844041e-05, ...,\n",
       "       0.00000000e+00, 3.53688082e-05, 1.76844041e-05])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First estimate for ham\n",
    "\n",
    "# summing up per feature count\n",
    "training_X_ham = training_X[training_y ==1]\n",
    "print(training_X_ham.shape)\n",
    "per_feature_count =np.sum(training_X_ham, axis = 0)\n",
    "print(per_feature_count.shape)\n",
    "\n",
    "#print(np.count_nonzero(per_feature_count))\n",
    "print(np.sum(per_feature_count))\n",
    "parameters_w_ham = per_feature_count/(np.sum(per_feature_count))\n",
    "\n",
    "parameters_w_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's estimate parameters for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 7353)\n",
      "[0.         0.00021219 0.         ... 0.00014146 0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "training_X_spam = training_X[training_y ==0]\n",
    "print(training_X_spam.shape)\n",
    "per_feature_count =np.sum(training_X_spam, axis = 0)\n",
    "per_feature_count.shape\n",
    "\n",
    "np.count_nonzero(per_feature_count)\n",
    "parameters_w_spam = per_feature_count/(np.sum(per_feature_count))\n",
    "print(parameters_w_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero probability issue\n",
    "As we can see some of the probablity can be zero. It will create problem when we estimate probability of a new document in test set if that word was not in training set. \n",
    "\n",
    "If any of the term in product is zero it will result in zero product. If any of the class don't have this term then probability of this document for any class will be zero. It is an ambiguous situation. If we play log trick for comparing product of probability, we will be in troble as log of zero is not defined too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to handle this situtation to add a fake 1 count of the word in each class. This is called Laplace law of sccession or add one smoothing.\n",
    "\n",
    "We estimate\n",
    "<font size = 8> \n",
    "$P(w_j|c) = \\frac{\\sum_{i=1}^N x_{ij}\\mathbb{1}(y_i=c) + 1}{\\sum_{k=1}^{D} \\sum_{i=1}^N x_{ik}\\mathbb{1}(y_i=c) + |V|}.$ \n",
    "</font>\n",
    "where $\\mathbb{1}$ is indicator function and $|V|$ is size of our dictionary.\n",
    "\n",
    "This can be done by adding a row of ones to training_X_ham and training_X_spam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New parameters Laplace law of sccession or add one smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate new parameter for ham class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4347, 7353)\n",
      "[2.80689936e-05 1.29117370e-03 2.80689936e-05 ... 1.40344968e-05\n",
      " 4.21034904e-05 2.80689936e-05]\n"
     ]
    }
   ],
   "source": [
    "training_X_ham = training_X[training_y ==1]\n",
    "training_X_ham = np.append(training_X_ham, np.ones(shape=(1, training_X_ham.shape[1])), axis=0)\n",
    "print(training_X_ham.shape)\n",
    "per_feature_count =np.sum(training_X_ham, axis = 0)\n",
    "per_feature_count.shape\n",
    "\n",
    "np.count_nonzero(per_feature_count)\n",
    "parameters_w_ham = per_feature_count/(np.sum(per_feature_count) + training_X_ham.shape[1])\n",
    "print(parameters_w_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate new parameter for spam class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(671, 7353)\n",
      "[3.46692553e-05 1.38677021e-04 3.46692553e-05 ... 1.04007766e-04\n",
      " 3.46692553e-05 3.46692553e-05]\n"
     ]
    }
   ],
   "source": [
    "training_X_spam = training_X[training_y ==0]\n",
    "training_X_spam = np.append(training_X_spam, np.ones(shape=(1, training_X_spam.shape[1])), axis=0)\n",
    "print(training_X_spam.shape)\n",
    "per_feature_count =np.sum(training_X_spam, axis = 0)\n",
    "per_feature_count.shape\n",
    "\n",
    "np.count_nonzero(per_feature_count)\n",
    "parameters_w_spam = per_feature_count/(np.sum(per_feature_count) + training_X_spam.shape[1])\n",
    "print(parameters_w_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate class probabilities P(c=ham) and P(c=spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8662813870067756, 0.1337186129932244)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham = training_X_ham.shape[0]/(training_X_ham.shape[0] + training_X_spam.shape[0])\n",
    "spam = 1- ham\n",
    "ham,spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4347\n"
     ]
    }
   ],
   "source": [
    "print(len(training_X_ham))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we have learned the model(i.e its parameters, probabilities of different words occuring in ham dice and spam dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How good is our model ?\n",
    "- Let take out our test data and convert to count feature vector using same dictionary\n",
    "- Calulate the probability if test data belonging to Ham or spam. i.e if probability if >=.5 Ham otherwise spam\n",
    " or we can calulate the ratio\n",
    " <font size = 5>\n",
    " $\\frac{P(x_{test}|c=ham)}{P(x_{test}|c=spam)} = \\frac{ P(c=ham)  \\prod^{D}_{j =1} P(w_j|c=ham)^{x_{test,j}}} { P(c= spam)\\prod^{D}_{j=1} P(w_j|c=spam)^{x_{test, j}}}$ \n",
    " </font>\n",
    " \n",
    " **Note:Generally such large product of probabilties, turns out to be zero because of computer representation limits of real numbers.**\n",
    " \n",
    " Another option is let take log on right hand side and after some manipulation one can show that if\n",
    " <font size = 5>\n",
    "  $\\sum_{j =1}^{D} (x_{test,j})log (P(w_j|c=ham)) +log(P(c= ham)) \\ge log(P(c=spam))+ \\sum_{j =1}^{D} (x_{test,j})log (P(w_j|c=spam))$\n",
    "  \n",
    "  </font>\n",
    "  \n",
    " then it is ham otherwise spam\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "- Compare our prediction of label with test data label and let's report accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(558, 7353)\n",
      "7353\n",
      "(558,)\n"
     ]
    }
   ],
   "source": [
    "test_x = np.zeros((len(test_messages), len(unique_word)), dtype=int)\n",
    "print(test_x.shape)\n",
    "print(len(word_to_index_dict))\n",
    "print(test_messages.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(558, 7353)\n"
     ]
    }
   ],
   "source": [
    "def build_feature(sms, word_to_index_dict):\n",
    "    feature = np.zeros((len(word_to_index_dict),), dtype=int)\n",
    "    word_freq =  Counter(sms)\n",
    "    # setting the word count in sms_no row of sms_features\n",
    "    for word, freq in word_freq.items():\n",
    "        if word in word_to_index_dict:\n",
    "            index_of_word = word_to_index_dict[word]\n",
    "            feature[index_of_word] = freq\n",
    "    return feature        \n",
    "    \n",
    "for sms_no, sms in enumerate(test_messages):\n",
    "    test_x[sms_no] = build_feature(sms, word_to_index_dict)\n",
    "\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Again checking feature creation/encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'hello': 1, 'they': 1, 'are': 1, 'going': 1, 'to': 1, 'the': 1, 'village': 1, 'pub': 1, 'at': 1, 'so': 1, 'either': 1, 'come': 1, 'here': 1, 'or': 1, 'there': 1, 'accordingly': 1, 'ok': 1})\n",
      "##Encoding for sms no 2 in feature vector is ##\n",
      "to 1\n",
      "pub 1\n",
      "here 1\n",
      "come 1\n",
      "village 1\n",
      "going 1\n",
      "either 1\n",
      "the 1\n",
      "ok 1\n",
      "or 1\n",
      "at 1\n",
      "so 1\n",
      "they 1\n",
      "accordingly 1\n",
      "there 1\n",
      "are 1\n",
      "hello 1\n"
     ]
    }
   ],
   "source": [
    "sms_no =2\n",
    "message_word_count = Counter(test_messages[sms_no])\n",
    "print(message_word_count)\n",
    "\n",
    "# Let' check non zero location in sms_features to see if count is set properly\n",
    "print('##Encoding for sms no {} in feature vector is ##'.format(sms_no))\n",
    "for i, count in enumerate(test_x[sms_no]):\n",
    "    if count >0:\n",
    "        print(index_to_word_dict[i], count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert ham and spam to 1 and 0 integer as done in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(558,)\n"
     ]
    }
   ],
   "source": [
    "# This is our test_y value\n",
    "test_y =(test_labels.values == 'ham').astype(int)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally let's calculate ham/spam probability for test messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((558,), (558,))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_score = np.zeros_like(test_y,dtype=float)\n",
    "spam_score = np.zeros_like(test_y,dtype=float)\n",
    "ham_score.shape, spam_score.shape# just printing to make sure shape is right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(parameters,test_sms, class_prior):\n",
    "    return np.sum(np.log(np.power(parameters,test_sms))) + class_prior\n",
    "\n",
    "for idx, test_sms in enumerate(test_x):# this will fetch row by row, encoded test messages\n",
    "    ham_score[idx] = calculate_score(parameters_w_ham,test_sms, np.log(ham))\n",
    "    spam_score[idx] = calculate_score(parameters_w_spam, test_sms, np.log(spam))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -64.13108259, -158.05199309]),\n",
       " array([ -80.92560929, -137.90110922]),\n",
       " array([1, 0]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let print some values for visual comparision/verification\n",
    "ham_score[0:2], spam_score[0:2], test_y[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the label ham(1) or spam(0)\n",
    "ham_or_spam = (ham_score >= spam_score).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0, 1, 0, 1]), array([1, 0, 1, 0, 1]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_or_spam[0:5], test_y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is 0.9802867383512545\n"
     ]
    }
   ],
   "source": [
    "accuracy= np.sum(ham_or_spam == test_y) / len(ham_or_spam)\n",
    "print('accuracy on test set is {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ham_or_spam(message):\n",
    "    feature = build_feature(message, word_to_index_dict)\n",
    "    ham_score = calculate_score(parameters_w_ham,feature, np.log(ham))\n",
    "    spam_score = calculate_score(parameters_w_spam, feature, np.log(spam))\n",
    "    \n",
    "    return 'ham' if ham_score > spam_score else 'spam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's see how it works on new spam message which is a modified  training message (coming from the same distribution as training messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ham_or_spam(' your mailbox messaging sm alert call back 09056242159 to retrieve your message'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'BlueViolet' size = 6> Comparing our model from scratch with inbuilt sklearn classifiers </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9802867383512545"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(training_X, training_y)\n",
    "ypred= classifier.predict(test_x)\n",
    "classifier.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\18482\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9802867383512545"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(training_X, training_y)\n",
    "ypred= classifier.predict(test_x)\n",
    "classifier.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kudos we have matched the accuracy of sklearn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
